#!/usr/bin/env python3
"""
Auto ALM Sync - Final
- Uses your tested authentication method (HTTPBasicAuth + auth.anaplan.com)
- Function-based, important comments only
- Logging, table UI (tabulate optional), size display MB/GB, parallel sync
Dependencies:
  pip install requests pwinput tabulate
"""

import os
import sys
import json
import time
import shutil
import logging
import base64
from datetime import datetime
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor
from requests.auth import HTTPBasicAuth

import requests
import pwinput

# Optional pretty table
try:
    from tabulate import tabulate
    HAVE_TABULATE = True
except Exception:
    HAVE_TABULATE = False

# -------------------------
# Constants
# -------------------------
CONFIG_FILE = "config.json"
LOG_BACKUP_DIR = "Log_Backup"
LOGS_DIR = "Logs"
MODEL_HISTORY_DIR = "Model_History"
LOG_FILENAME_PREFIX = "Auto_ALM"
ANAPLAN_AUTH_URL = "https://auth.anaplan.com/token/authenticate"
ANAPLAN_API_BASE = "https://api.anaplan.com/2/0"
REQUEST_TIMEOUT = 30
MAX_WORKERS = 6  # threads for parallel sync

# -------------------------
# Utilities
# -------------------------
def timestamp_str() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def bytes_to_human(nbytes: int) -> str:
    one_gb = 1024 ** 3
    one_mb = 1024 ** 2
    if nbytes < one_gb:
        mb = nbytes / one_mb
        return f"{mb:,.2f} MB"
    else:
        gb = nbytes / one_gb
        return f"{gb:,.2f} GB"

# -------------------------
# Logging & archival
# -------------------------
def archive_existing_logs(main_dir: str = ".", backup_dir: str = LOG_BACKUP_DIR) -> List[str]:
    moved = []
    ensure_dir(backup_dir)
    # notify if created (empty)
    if not os.listdir(backup_dir):
        print(f"Created folder: {backup_dir}")
    for entry in os.listdir(main_dir):
        path = os.path.join(main_dir, entry)
        if os.path.isfile(path) and entry.lower().endswith(".log"):
            dest = os.path.join(backup_dir, entry)
            shutil.move(path, dest)
            print(f"Moved log file '{entry}' -> '{backup_dir}'")
            moved.append(dest)
    return moved

def create_new_log(logs_dir: str = LOGS_DIR) -> Tuple[str, logging.Logger]:
    ensure_dir(logs_dir)
    fname = f"{LOG_FILENAME_PREFIX}_{timestamp_str()}.log"
    filepath = os.path.join(logs_dir, fname)

    logger = logging.getLogger("auto_alm")
    logger.setLevel(logging.DEBUG)
    # clear handlers
    if logger.handlers:
        logger.handlers.clear()

    fh = logging.FileHandler(filepath)
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(fh)

    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("%(message)s"))
    logger.addHandler(ch)

    logger.info(f"Log file created: {filepath}")
    return filepath, logger

# -------------------------
# Config
# -------------------------
def load_config(path: str = CONFIG_FILE) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def parse_model_pairs(config: dict) -> Tuple[str, List[Dict[str, str]]]:
    md = config.get("Model Details", {})
    export_action_name = md.get("export_action_name", "").strip()
    pairs = []
    for entry in md.get("model_ids", []):
        dev = entry.get("dev_model_id")
        prod = entry.get("prod_model_id")
        if dev and prod:
            pairs.append({"dev": dev, "prod": prod})
    return export_action_name, pairs

# -------------------------
# Authentication (user-provided working code)
# -------------------------
def authentication(username: str, password: str) -> str:
    """
    Use user's tested auth method. Returns auth token string.
    Raises on failure.
    """
    resp = requests.post(ANAPLAN_AUTH_URL, auth=HTTPBasicAuth(username, password), timeout=REQUEST_TIMEOUT)
    resp.raise_for_status()
    j = resp.json()
    token = j["tokenInfo"]["tokenValue"]
    return token

def prompt_credentials_and_auth(logger: logging.Logger) -> str:
    username = input("Enter your Anaplan Username/Email: ").strip()
    password = pwinput.pwinput("Enter your Anaplan password: ")
    try:
        token = authentication(username, password)
        logger.info("Authentication successful for user (username logged, password not logged).")
        print(f"User '{username}' logged in successfully.")
        return token
    except Exception as e:
        logger.error(f"Authentication failed: {e}")
        print("Authentication failed. Please verify your username/password and try again.")
        raise

# -------------------------
# Workspace & model discovery
# -------------------------
def get_all_workspaces(token: str, logger: logging.Logger) -> List[Dict[str, Any]]:
    headers = {"Authorization": f"AnaplanAuthToken {token}", "Accept": "application/json"}
    url = f"{ANAPLAN_API_BASE}/workspaces"
    try:
        r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        return r.json().get("workspaces") or r.json().get("items") or []
    except requests.RequestException as e:
        logger.warning(f"Could not list workspaces: {e}")
        return []

def get_models_in_workspace(token: str, workspace_id: str, logger: logging.Logger) -> List[Dict[str, Any]]:
    headers = {"Authorization": f"AnaplanAuthToken {token}", "Accept": "application/json"}
    url = f"{ANAPLAN_API_BASE}/workspaces/{workspace_id}/models"
    try:
        r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        return r.json().get("models") or r.json().get("items") or []
    except requests.RequestException as e:
        logger.warning(f"Could not list models in workspace {workspace_id}: {e}")
        return []

def discover_model_metadata(token: str, model_ids: List[str], logger: logging.Logger) -> Dict[str, Dict[str, Any]]:
    """
    Return mapping model_id -> {model_name, workspace_id, workspace_name}
    """
    result = {}
    workspaces = get_all_workspaces(token, logger)
    for ws in workspaces:
        ws_id = ws.get("id") or ws.get("workspaceId")
        ws_name = ws.get("name")
        if not ws_id:
            continue
        models = get_models_in_workspace(token, ws_id, logger)
        for m in models:
            mid = m.get("id") or m.get("modelId")
            if mid and mid in model_ids:
                result[mid] = {
                    "model_name": m.get("name") or "(unknown)",
                    "workspace_id": ws_id,
                    "workspace_name": ws_name or "(unknown)"
                }
    for mid in model_ids:
        if mid not in result:
            result[mid] = {"model_name": "(unknown)", "workspace_id": None, "workspace_name": "(unknown)"}
    return result

# -------------------------
# Export / Model History
# -------------------------
def find_export_id_by_name(token: str, model_id: str, export_name: str, logger: logging.Logger) -> str:
    headers = {"Authorization": f"AnaplanAuthToken {token}", "Accept": "application/json"}
    url = f"{ANAPLAN_API_BASE}/models/{model_id}/exports"
    r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()
    data = r.json()
    exports = data.get("exports") or data.get("items") or []
    for e in exports:
        if e.get("name") == export_name:
            return e.get("id")
    raise RuntimeError(f"Export action '{export_name}' not found for model {model_id}")

def run_export_and_download(token: str, workspace_id: str, model_id: str, export_id: str, out_dir: str, logger: logging.Logger) -> str:
    headers = {"Authorization": f"AnaplanAuthToken {token}", "Accept": "application/json"}
    start_url = f"{ANAPLAN_API_BASE}/workspaces/{workspace_id}/models/{model_id}/exports/{export_id}/tasks"
    r = requests.post(start_url, headers=headers, json={"localeName": "en_US"}, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()
    task_info = r.json()
    # attempt to get task id
    task_id = None
    if isinstance(task_info, dict):
        task_id = (task_info.get("task") or {}).get("id") or task_info.get("taskId") or task_info.get("id")
    if not task_id:
        # fallback: some tenants return tasks differently
        raise RuntimeError("Could not start export task (task id missing).")
    task_url = f"{start_url}/{task_id}"
    # poll for completion
    for _ in range(60):
        time.sleep(2)
        t_resp = requests.get(task_url, headers=headers, timeout=REQUEST_TIMEOUT)
        if t_resp.status_code != 200:
            continue
        tdata = t_resp.json()
        status = (tdata.get("task") or {}).get("status") or tdata.get("status") or ""
        if isinstance(status, str) and status.lower() in ("completed", "success"):
            break
    # download
    download_url = f"{ANAPLAN_API_BASE}/workspaces/{workspace_id}/models/{model_id}/exports/{export_id}/result"
    dl = requests.get(download_url, headers=headers, timeout=REQUEST_TIMEOUT, stream=True)
    dl.raise_for_status()
    ensure_dir(out_dir)
    fname = f"{model_id}_{timestamp_str()}.zip"
    fpath = os.path.join(out_dir, fname)
    with open(fpath, "wb") as fh:
        for chunk in dl.iter_content(chunk_size=8192):
            if chunk:
                fh.write(chunk)
    logger.info(f"Downloaded model history: {fpath}")
    return fpath

# -------------------------
# Revision tag helpers
# -------------------------
def list_revision_tags(token: str, model_id: str, logger: logging.Logger) -> List[Dict[str, Any]]:
    headers = {"Authorization": f"AnaplanAuthToken {token}", "Accept": "application/json"}
    url = f"{ANAPLAN_API_BASE}/models/{model_id}/revisions"
    try:
        r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        data = r.json()
        return data.get("revisions") or data.get("items") or []
    except requests.RequestException:
        logger.warning("Could not fetch revision tags; endpoint may vary.")
        return []

def create_revision_tag(token: str, model_id: str, tag_name: str, logger: logging.Logger) -> Dict[str, Any]:
    headers = {"Authorization": f"AnaplanAuthToken {token}", "Content-Type": "application/json"}
    url = f"{ANAPLAN_API_BASE}/models/{model_id}/revisions"
    payload = {"name": tag_name}
    r = requests.post(url, headers=headers, json=payload, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()
    return r.json()

# -------------------------
# Workspace size & estimate
# -------------------------
def get_workspace_usage(token: str, workspace_id: str, logger: logging.Logger) -> Tuple[int, int]:
    if not workspace_id:
        return 0, 0
    headers = {"Authorization": f"AnaplanAuthToken {token}", "Accept": "application/json"}
    url = f"{ANAPLAN_API_BASE}/workspaces/{workspace_id}/usage"
    try:
        r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        d = r.json()
        used = d.get("used") or d.get("usedBytes") or d.get("usedSpace") or 0
        alloc = d.get("allocated") or d.get("allocatedBytes") or d.get("allocatedSpace") or 0
        return int(used), int(alloc)
    except requests.RequestException:
        logger.warning(f"Could not fetch usage for workspace {workspace_id}.")
        return 0, 0

def estimate_post_sync_size(token: str, prod_info: Dict[str, Any], revision_file_path: str, logger: logging.Logger) -> Dict[str, Any]:
    ws = prod_info.get("workspace_id")
    used, alloc = get_workspace_usage(token, ws, logger)
    revision_size = 0
    if revision_file_path and os.path.exists(revision_file_path):
        try:
            revision_size = os.path.getsize(revision_file_path)
        except Exception:
            revision_size = 0
    after = used + revision_size
    pct_after = (after / alloc) if alloc and alloc > 0 else 0.0
    return {"current": used, "alloc": alloc, "revision_size": revision_size, "after": after, "pct_after": pct_after}

# -------------------------
# UX: table
# -------------------------
def pretty_table(dev_info: Dict[str, str], prod_info: Dict[str, str]) -> str:
    headers = ["Info", "DEV", "PROD"]
    rows = [
        ["Model ID", dev_info.get("model_id"), prod_info.get("model_id")],
        ["Model Name", dev_info.get("model_name"), prod_info.get("model_name")],
        ["Workspace", dev_info.get("workspace_name"), prod_info.get("workspace_name")]
    ]
    if HAVE_TABULATE:
        return tabulate(rows, headers=headers, tablefmt="pretty")
    else:
        # fallback simple table
        col1w = 18
        colw = 36
        sep = "+" + "-"*(col1w+2) + "+" + "-"*(colw+2) + "+" + "-"*(colw+2) + "+"
        lines = [sep, "| {:<{}} | {:<{}} | {:<{}} |".format("Info", col1w, "DEV", colw, "PROD", colw), sep]
        for r in rows:
            lines.append("| {:<{}} | {:<{}} | {:<{}} |".format(r[0], col1w, str(r[1])[:colw], colw, str(r[2])[:colw], colw))
        lines.append(sep)
        return "\n".join(lines)

# -------------------------
# Per-pair interactive flow
# -------------------------
def ask_revision_choice_for_pair(idx: int, dev_info: Dict[str, Any], prod_info: Dict[str, Any],
                                 token: str, export_action_name: str, logger: logging.Logger,
                                 revision_file_path: str) -> Dict[str, Any]:
    print("\n" + "="*64)
    print(f"Pair #{idx+1}")
    print(pretty_table(dev_info, prod_info))
    print("→ Processing this Dev → Prod pair...\n")

    print("Revision Tag options:")
    print("  1) Use latest available Revision Tag")
    print("  2) Create a new Revision Tag now")
    print("  3) List available Revision Tags and select one")
    choice = input("Enter choice (1/2/3) [1]: ").strip() or "1"

    selected_tag = None
    if choice == "1":
        tags = list_revision_tags(token, dev_info["model_id"], logger)
        if tags:
            selected_tag = tags[0].get("name") or tags[0].get("id")
            print(f"Selected latest tag: {selected_tag}")
        else:
            print("No tags found; switching to create new tag.")
            choice = "2"

    if choice == "2":
        tag_name = input("Enter name for new revision tag (blank => auto): ").strip()
        if not tag_name:
            tag_name = f"AutoTag_{timestamp_str()}"
            print(f"Using auto name: {tag_name}")
        try:
            created = create_revision_tag(token, dev_info["model_id"], tag_name, logger)
            selected_tag = created.get("name") or created.get("id") or tag_name
            print(f"Created tag: {selected_tag}")
        except Exception:
            print("Failed to create tag via API; storing provided name.")
            selected_tag = tag_name

    if choice == "3":
        tags = list_revision_tags(token, dev_info["model_id"], logger)
        if not tags:
            print("No tags found; switching to create new tag.")
            choice = "2"
            tag_name = input("Enter name for new revision tag (blank => auto): ").strip()
            if not tag_name:
                tag_name = f"AutoTag_{timestamp_str()}"
            selected_tag = tag_name
        else:
            for i, t in enumerate(tags):
                print(f"  {i+1}) {t.get('name') or t.get('id')}")
            sel = input("Choose number (or Enter to cancel): ").strip()
            try:
                idx_choice = int(sel) - 1
                selected_tag = tags[idx_choice].get("name") or tags[idx_choice].get("id")
            except Exception:
                print("Invalid selection; defaulting to latest if available.")
                if tags:
                    selected_tag = tags[0].get("name") or tags[0].get("id")

    # size estimation
    size_info = estimate_post_sync_size(token, prod_info, revision_file_path, logger)
    curr = size_info["current"]
    alloc = size_info["alloc"]
    after = size_info["after"]
    pct_after = size_info["pct_after"]

    print("\nWorkspace Size Check for PROD:", prod_info.get("model_name"))
    print("-"*48)
    print(f"Before Sync : {bytes_to_human(curr)} / {bytes_to_human(alloc) if alloc else '(unknown)'}")
    print(f"After Sync  : {bytes_to_human(after)} ({pct_after:.2%} of allocation)" if alloc else f"After Sync  : {bytes_to_human(after)} (allocation unknown)")

    proceed = True
    if alloc and pct_after > 0.95:
        ans = input("Estimated usage >95%. Proceed with sync for this Prod? (y/n) [n]: ").strip().lower() or "n"
        proceed = ans == "y"
    else:
        print("Estimated usage after sync is acceptable.")

    return {
        "dev": dev_info["model_id"],
        "prod": prod_info["model_id"],
        "dev_name": dev_info.get("model_name"),
        "prod_name": prod_info.get("model_name"),
        "dev_ws": dev_info.get("workspace_id"),
        "prod_ws": prod_info.get("workspace_id"),
        "choice": choice,
        "tag": selected_tag,
        "proceed": proceed,
        "size_info": size_info,
        "revision_file": revision_file_path
    }

# -------------------------
# Sync execution
# -------------------------
def promote_revision_to_prod(task: Dict[str, Any], token: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    Representative promote call - update endpoint/payload if your ALM API differs.
    """
    dev = task["dev"]
    prod = task["prod"]
    prod_ws = task.get("prod_ws")
    tag = task.get("tag")
    headers = {"Authorization": f"AnaplanAuthToken {token}", "Content-Type": "application/json"}
    # Representative endpoint (update if your tenant uses different ALM path)
    url = f"{ANAPLAN_API_BASE}/workspaces/{prod_ws}/models/{prod}/revisions/promote"
    payload = {"sourceModelId": dev, "revisionName": tag}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=REQUEST_TIMEOUT)
        if r.status_code in (200, 201, 202):
            logger.info(f"Promote request accepted for Prod={prod} (tag={tag})")
            return {"dev": dev, "prod": prod, "status": "started", "response": r.text}
        else:
            logger.error(f"Promote failed for Prod={prod}: {r.status_code} {r.text}")
            return {"dev": dev, "prod": prod, "status": "failed", "response": r.text}
    except requests.RequestException as e:
        logger.error(f"Promote error for Prod={prod}: {e}")
        return {"dev": dev, "prod": prod, "status": "error", "detail": str(e)}

def parallel_sync_executor(sync_tasks: List[Dict[str, Any]], token: str, logger: logging.Logger, max_workers: int = MAX_WORKERS) -> List[Dict[str, Any]]:
    to_run = [t for t in sync_tasks if t.get("proceed")]
    if not to_run:
        logger.info("No tasks confirmed for sync.")
        return []
    logger.info(f"Starting parallel sync for {len(to_run)} tasks with {max_workers} workers...")
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        # Use executor.map to run tasks in parallel and avoid explicit for-loop control for syncing
        mapped = ex.map(lambda t: promote_revision_to_prod(t, token, logger), to_run)
        for r in mapped:
            results.append(r)
    logger.info("Parallel sync completed.")
    return results

# -------------------------
# Main
# -------------------------
def main():
    # 1) archive & create log
    archive_existing_logs(".")
    logpath, logger = create_new_log(LOGS_DIR)

    # 2) prompt credentials & authenticate (user-provided method)
    try:
        token = prompt_credentials_and_auth(logger)
    except Exception:
        logger.error("Authentication failed - exiting.")
        return

    # 3) load config and parse pairs
    try:
        conf = load_config(CONFIG_FILE)
        export_action_name, pairs = parse_model_pairs(conf)
        if not pairs:
            logger.error("No pairs found in config - exiting.")
            return
        logger.info(f"Found {len(pairs)} dev->prod pairs.")
    except Exception as e:
        logger.error(f"Failed to read config: {e}")
        return

    # 4) discover model metadata
    all_model_ids = list({m["dev"] for m in pairs} | {m["prod"] for m in pairs})
    discovery = discover_model_metadata(token, all_model_ids, logger)

    # 5) iterate pairs - download history & collect choices
    sync_plan = []
    for idx, pair in enumerate(pairs):
        dev = pair["dev"]
        prod = pair["prod"]
        dev_meta = {
            "model_id": dev,
            "model_name": discovery.get(dev, {}).get("model_name"),
            "workspace_id": discovery.get(dev, {}).get("workspace_id"),
            "workspace_name": discovery.get(dev, {}).get("workspace_name")
        }
        prod_meta = {
            "model_id": prod,
            "model_name": discovery.get(prod, {}).get("model_name"),
            "workspace_id": discovery.get(prod, {}).get("workspace_id"),
            "workspace_name": discovery.get(prod, {}).get("workspace_name")
        }

        # Print table for UX
        print("\n" + "="*60)
        print(f"Pair #{idx+1}: {dev_meta.get('model_name')} -> {prod_meta.get('model_name')}")
        print(pretty_table(dev_meta, prod_meta))

        # Download model history for dev using export action from config (if possible)
        mh_file = ""
        try:
            if export_action_name:
                logger.info(f"Downloading model history for dev model {dev} using export '{export_action_name}'")
                # Need workspace id for dev
                dev_ws = dev_meta.get("workspace_id")
                if dev_ws:
                    export_id = find_export_id_by_name(token, dev, export_action_name, logger)
                    mh_file = run_export_and_download(token, dev_ws, dev, export_id, MODEL_HISTORY_DIR, logger)
                else:
                    logger.warning(f"Workspace id unknown for dev model {dev}; skipping download.")
            else:
                logger.warning("No export_action_name provided in config; skipping model history download.")
        except Exception as e:
            logger.warning(f"Model history download failed for {dev}: {e}")
            mh_file = ""

        # Interactively ask revision/tag choice and whether to proceed
        try:
            details = ask_revision_choice_for_pair(idx, dev_meta, prod_meta, token, export_action_name, logger, mh_file)
            sync_plan.append(details)
        except Exception as e:
            logger.error(f"Error during interactive choices for pair {dev}->{prod}: {e}")
            sync_plan.append({
                "dev": dev, "prod": prod, "tag": None, "proceed": False, "revision_file": mh_file
            })

    # 6) Show final plan summary & ask final confirmation
    print("\n" + "="*60)
    print("Final Sync Plan Summary")
    for i, s in enumerate(sync_plan, start=1):
        status = "WILL SYNC" if s.get("proceed") else "SKIPPED"
        tag = s.get("tag") or "(none)"
        print(f"{i}. {s.get('dev_name') or s.get('dev')} -> {s.get('prod_name') or s.get('prod')} | Tag: {tag} | {status}")
    proceed_all = input("\nProceed to run syncs for items marked WILL SYNC? (y/n) [y]: ").strip().lower() or "y"
    if proceed_all != "y":
        logger.info("User cancelled final sync execution.")
        return

    # 7) Execute parallel syncs
    results = parallel_sync_executor(sync_plan, token, logger)

    # 8) Log results & exit
    logger.info("---- SUMMARY ----")
    if not results:
        logger.info("No syncs executed.")
    else:
        for r in results:
            logger.info(json.dumps(r))
    logger.info("Script finished.")

if __name__ == "__main__":
    main()
